{
	"jobConfig": {
		"name": "json2csv_spark.py",
		"description": "",
		"role": "arn:aws:iam::084828603834:role/service-role/AWSGlueServiceRole-ForODITransformation",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "json2csv_spark.py",
		"scriptLocation": "s3://my-raw-odi-bucket/odis/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--ACTION_FILTER",
				"value": "uploaded",
				"existing": false
			},
			{
				"key": "--COALESCE",
				"value": "1",
				"existing": false
			},
			{
				"key": "--FAIL_ON_UNMATCHED",
				"value": "true",
				"existing": false
			},
			{
				"key": "--INPUT_GLOB",
				"value": "s3://my-raw-odi-bucket/odis/raw/*.json",
				"existing": false
			},
			{
				"key": "--MANIFEST_GLOB",
				"value": "s3://my-raw-odi-bucket/odis/raw/_manifest/ingest_date=*/manifest_*.jsonl",
				"existing": false
			},
			{
				"key": "--OUTPUT_BASE",
				"value": "s3://my-raw-odi-bucket/odis/csv/",
				"existing": false
			},
			{
				"key": "--READ_MODE",
				"value": "latest",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-08-06T20:25:24.140Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-084828603834-eu-north-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-084828603834-eu-north-1/sparkHistoryLogs/",
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "# glue_pyspark_app.py  (Glue 4.0/5.0, Spark 3.x)\r\n\r\nimport sys\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom pyspark.sql import functions as F, types as T\r\nfrom pyspark.sql.window import Window\r\n\r\n# ---------- ARGS ----------\r\n# --INPUT_GLOB s3://my-raw-odi-bucket/odis/raw/*.json\r\n# --MANIFEST_GLOB s3://my-raw-odi-bucket/odis/raw/_manifest/ingest_date=*/manifest_*.jsonl\r\n# --OUTPUT_BASE s3://my-raw-odi-bucket/odis/csv/\r\n# --COALESCE 1\r\n# --READ_MODE latest\r\n# --ACTION_FILTER uploaded\r\n# --FAIL_ON_UNMATCHED true\r\nargs = getResolvedOptions(\r\n    sys.argv,\r\n    [\"INPUT_GLOB\", \"MANIFEST_GLOB\", \"OUTPUT_BASE\", \"COALESCE\",\r\n     \"READ_MODE\", \"ACTION_FILTER\", \"FAIL_ON_UNMATCHED\"]\r\n)\r\nINPUT_GLOB = args[\"INPUT_GLOB\"]\r\nMANIFEST_GLOB = args[\"MANIFEST_GLOB\"]\r\nOUTPUT_BASE = args[\"OUTPUT_BASE\"].rstrip(\"/\") + \"/\"\r\nCOALESCE = int(args[\"COALESCE\"])\r\nREAD_MODE = args[\"READ_MODE\"].lower()          # latest | all\r\nACTION_FILTER = args[\"ACTION_FILTER\"].lower()  # uploaded | skipped | any\r\nFAIL_ON_UNMATCHED = args[\"FAIL_ON_UNMATCHED\"].lower() == \"true\"\r\n\r\n# ---------- SPARK/GLUE ----------\r\nglueContext = GlueContext(SparkContext.getOrCreate())\r\nspark = glueContext.spark_session\r\nspark.sparkContext.setLogLevel(\"ERROR\")\r\n\r\n# ---------- SCHEMA ----------\r\nwicket_fielder_schema = T.StructType([ T.StructField(\"name\", T.StringType(), True) ])\r\nwicket_schema = T.StructType([\r\n    T.StructField(\"player_out\", T.StringType(), True),\r\n    T.StructField(\"kind\", T.StringType(), True),\r\n    T.StructField(\"fielders\", T.ArrayType(wicket_fielder_schema), True),\r\n])\r\nruns_schema = T.StructType([\r\n    T.StructField(\"batter\", T.IntegerType(), True),\r\n    T.StructField(\"extras\", T.IntegerType(), True),\r\n    T.StructField(\"total\", T.IntegerType(), True),\r\n])\r\ndelivery_schema = T.StructType([\r\n    T.StructField(\"bowler\", T.StringType(), True),\r\n    T.StructField(\"batter\", T.StringType(), True),\r\n    T.StructField(\"non_striker\", T.StringType(), True),\r\n    T.StructField(\"runs\", runs_schema, True),\r\n    T.StructField(\"wickets\", T.ArrayType(wicket_schema), True),\r\n])\r\nover_schema = T.StructType([\r\n    T.StructField(\"over\", T.IntegerType(), True),\r\n    T.StructField(\"deliveries\", T.ArrayType(delivery_schema), True),\r\n])\r\ninning_schema = T.StructType([\r\n    T.StructField(\"team\", T.StringType(), True),\r\n    T.StructField(\"overs\", T.ArrayType(over_schema), True),\r\n])\r\nevent_schema = T.StructType([\r\n    T.StructField(\"name\", T.StringType(), True),\r\n    T.StructField(\"match_number\", T.StringType(), True),\r\n    T.StructField(\"stage\", T.StringType(), True),\r\n])\r\noutcome_schema = T.StructType([\r\n    T.StructField(\"winner\", T.StringType(), True),\r\n    T.StructField(\"result\", T.StringType(), True),\r\n])\r\ntoss_schema = T.StructType([\r\n    T.StructField(\"winner\", T.StringType(), True),\r\n    T.StructField(\"decision\", T.StringType(), True),\r\n])\r\ninfo_schema = T.StructType([\r\n    T.StructField(\"match_type_number\", T.LongType(), True),\r\n    T.StructField(\"event\", event_schema, True),\r\n    T.StructField(\"dates\", T.ArrayType(T.StringType()), True),\r\n    T.StructField(\"match_type\", T.StringType(), True),\r\n    T.StructField(\"season\", T.StringType(), True),\r\n    T.StructField(\"team_type\", T.StringType(), True),\r\n    T.StructField(\"overs\", T.IntegerType(), True),\r\n    T.StructField(\"city\", T.StringType(), True),\r\n    T.StructField(\"venue\", T.StringType(), True),\r\n    T.StructField(\"gender\", T.StringType(), True),\r\n    T.StructField(\"teams\", T.ArrayType(T.StringType()), True),\r\n    T.StructField(\"outcome\", outcome_schema, True),\r\n    T.StructField(\"toss\", toss_schema, True),\r\n    T.StructField(\"players\", T.MapType(T.StringType(), T.ArrayType(T.StringType())), True),\r\n])\r\nroot_schema = T.StructType([\r\n    T.StructField(\"info\", info_schema, True),\r\n    T.StructField(\"innings\", T.ArrayType(inning_schema), True),\r\n])\r\n\r\n# ---------- READ MANIFEST ----------\r\nmanifest_raw = (\r\n    spark.read.format(\"json\")\r\n         .load(MANIFEST_GLOB)\r\n         .withColumn(\"manifest_path\", F.input_file_name())\r\n         .select(\r\n             F.col(\"s3_uri\"),\r\n             F.col(\"stg_file_name\"),\r\n             F.col(\"stg_file_hashkey\"),\r\n             F.col(\"ingested_at\"),\r\n             F.col(\"action\"),\r\n             F.col(\"manifest_path\")\r\n         )\r\n)\r\n\r\n# keep only latest manifest if requested\r\nif READ_MODE == \"latest\":\r\n    latest_manifest_path = manifest_raw.agg(F.max(\"manifest_path\").alias(\"p\")).collect()[0][\"p\"]\r\n    manifest_raw = manifest_raw.filter(F.col(\"manifest_path\") == F.lit(latest_manifest_path))\r\n\r\n# filter by action if requested\r\nif ACTION_FILTER in (\"uploaded\", \"skipped\"):\r\n    manifest_raw = manifest_raw.filter(F.col(\"action\") == ACTION_FILTER)\r\n\r\n# materialize latest row per s3_uri (in case a manifest has duplicates)\r\nw = Window.partitionBy(\"s3_uri\").orderBy(F.col(\"ingested_at\").desc_nulls_last())\r\nmanifest_df = (\r\n    manifest_raw\r\n        .withColumn(\"rn\", F.row_number().over(w))\r\n        .where(F.col(\"rn\") == 1)\r\n        .drop(\"rn\")\r\n        .withColumnRenamed(\"s3_uri\", \"_source\")\r\n)\r\n\r\n# ---------- READ JSON ----------\r\nif READ_MODE == \"latest\":\r\n    # Incremental: only read files listed by the latest manifest (and action filter)\r\n    paths = [r[\"_source\"] for r in manifest_df.select(\"_source\").distinct().collect()]\r\n    if len(paths) == 0:\r\n        raise RuntimeError(\"No files to process from the selected manifest (check ACTION_FILTER / manifest date).\")\r\n    json_df = (\r\n        spark.read.format(\"json\")\r\n             .schema(root_schema)\r\n             .option(\"multiLine\", True)\r\n             .load(paths)\r\n             .withColumn(\"_source\", F.input_file_name())\r\n    )\r\nelse:\r\n    # Full refresh: read all JSONs via glob\r\n    json_df = (\r\n        spark.read.format(\"json\")\r\n             .schema(root_schema)\r\n             .option(\"multiLine\", True)\r\n             .load(INPUT_GLOB)\r\n             .withColumn(\"_source\", F.input_file_name())\r\n    )\r\n\r\n# ---------- JOIN MANIFEST → STAGING COLUMNS ----------\r\njson_df = (\r\n    json_df.join(manifest_df, on=\"_source\", how=\"left\")\r\n           .withColumn(\r\n               \"stg_file_name\",\r\n               F.coalesce(\r\n                   F.col(\"stg_file_name\"),\r\n                   F.regexp_extract(F.col(\"_source\"), r'([^/\\\\]+)$', 1)\r\n               )\r\n           )\r\n           .withColumn(\"stg_file_row_number\", F.lit(1))\r\n           .withColumn(\"stg_modified_ts\", F.current_timestamp())\r\n)\r\n\r\njson_df = json_df.withColumn(\"ingest_date\", F.to_date(F.col(\"ingested_at\")))\r\n\r\n# ---------- SANITY CHECK ----------\r\nif READ_MODE == \"all\" and FAIL_ON_UNMATCHED:\r\n    unmatched_cnt = json_df.filter(F.col(\"stg_file_hashkey\").isNull()).limit(1).count()\r\n    if unmatched_cnt > 0:\r\n        raise RuntimeError(\r\n            \"Some JSON files had no manifest row. \"\r\n            \"Either widen MANIFEST_GLOB or set --FAIL_ON_UNMATCHED false.\"\r\n        )\r\n\r\n# ---------- DERIVED ----------\r\nevent_date_str = F.element_at(F.col(\"info.dates\"), 1)\r\nevent_date = F.to_date(event_date_str, \"yyyy-MM-dd\")\r\n\r\nmatch_stage = F.coalesce(\r\n    F.col(\"info.event.match_number\").cast(\"string\"),\r\n    F.col(\"info.event.stage\").cast(\"string\"),\r\n    F.lit(\"NA\")\r\n)\r\n\r\nmatch_result = (\r\n    F.when(F.col(\"info.outcome.winner\").isNotNull(), F.lit(\"Result Declared\"))\r\n     .when(F.col(\"info.outcome.result\") == \"tie\", F.lit(\"Tie\"))\r\n     .when(F.col(\"info.outcome.result\") == \"no result\", F.lit(\"No Result\"))\r\n     .otherwise(F.col(\"info.outcome.result\"))\r\n)\r\n\r\ntoss_decision_cap = F.when(\r\n    F.col(\"info.toss.decision\").isNotNull(),\r\n    F.initcap(F.col(\"info.toss.decision\"))\r\n)\r\n\r\n# ---------- MATCH TABLE ----------\r\nmatch_table_df = json_df.select(\r\n    F.col(\"info.match_type_number\").alias(\"match_type_number\"),\r\n    F.col(\"info.event.name\").alias(\"event_name\"),\r\n    match_stage.alias(\"match_stage\"),\r\n    event_date.alias(\"event_date\"),\r\n    F.year(event_date).alias(\"event_year\"),\r\n    F.month(event_date).alias(\"event_month\"),\r\n    F.dayofmonth(event_date).alias(\"event_day\"),\r\n    F.col(\"info.match_type\").alias(\"match_type\"),\r\n    F.col(\"info.season\").alias(\"season\"),\r\n    F.col(\"info.team_type\").alias(\"team_type\"),\r\n    F.col(\"info.overs\").alias(\"overs\"),\r\n    F.col(\"info.city\").alias(\"city\"),\r\n    F.col(\"info.venue\").alias(\"venue\"),\r\n    F.col(\"info.gender\").alias(\"gender\"),\r\n    F.element_at(F.col(\"info.teams\"), 1).alias(\"first_team\"),\r\n    F.element_at(F.col(\"info.teams\"), 2).alias(\"second_team\"),\r\n    match_result.alias(\"match_result\"),\r\n    F.coalesce(F.col(\"info.outcome.winner\"), F.lit(\"NA\")).alias(\"winner\"),\r\n    F.col(\"info.toss.winner\").alias(\"toss_winner\"),\r\n    toss_decision_cap.alias(\"toss_decision\"),\r\n    F.col(\"stg_file_name\"),\r\n    F.col(\"stg_file_row_number\"),\r\n    F.col(\"stg_file_hashkey\"),\r\n    F.col(\"stg_modified_ts\"),\r\n    F.col(\"ingest_date\")\r\n)\r\n\r\n# ---------- PLAYER TABLE ----------\r\nplayers_exploded = (\r\n    json_df\r\n    .select(\r\n        F.col(\"info.match_type_number\").alias(\"match_type_number\"),\r\n        F.explode_outer(F.col(\"info.players\")).alias(\"country\", \"players_arr\"),\r\n        \"stg_file_name\", \"stg_file_row_number\", \"stg_file_hashkey\", \"stg_modified_ts\", \"ingest_date\"\r\n    )\r\n    .withColumn(\"player_name\", F.explode_outer(\"players_arr\"))\r\n    .select(\r\n        \"match_type_number\", \"country\", \"player_name\",\r\n        \"stg_file_name\", \"stg_file_row_number\", \"stg_file_hashkey\", \"stg_modified_ts\", \"ingest_date\"\r\n    )\r\n)\r\nplayer_table_df = players_exploded\r\n\r\n# ---------- DELIVERY TABLE ----------\r\nbase_deliveries = (\r\n    json_df\r\n    .select(\r\n        F.col(\"info.match_type_number\").alias(\"match_type_number\"),\r\n        F.posexplode_outer(\"innings\").alias(\"innings_index\", \"inn\")   # <-- capture index\r\n    )\r\n    .withColumn(\"innings_number\", F.col(\"innings_index\") + F.lit(1))  # convert 0-based → 1-based\r\n    .select(\r\n        \"match_type_number\",\r\n        \"innings_number\",\r\n        F.col(\"inn.team\").alias(\"country\"),\r\n        F.explode_outer(F.col(\"inn.overs\")).alias(\"ovr\")\r\n    )\r\n    .select(\r\n        \"match_type_number\",\r\n        \"innings_number\",\r\n        \"country\",\r\n        F.col(\"ovr.over\").alias(\"over\"),\r\n        F.posexplode_outer(F.col(\"ovr.deliveries\")).alias(\"ball_idx_zero\", \"del\")\r\n    )\r\n    .withColumn(\"ball_in_over\", F.col(\"ball_idx_zero\") + F.lit(1))\r\n    .select(\r\n        \"match_type_number\",\r\n        \"innings_number\",\r\n        \"country\",\r\n        \"over\",\r\n        \"ball_in_over\",\r\n        F.col(\"del.bowler\").alias(\"bowler\"),\r\n        F.col(\"del.batter\").alias(\"batter\"),\r\n        F.col(\"del.non_striker\").alias(\"non_striker\"),\r\n        F.col(\"del.runs.batter\").alias(\"runs\"),\r\n        F.col(\"del.runs.extras\").alias(\"extras\"),\r\n        F.col(\"del.runs.total\").alias(\"total\"),\r\n        F.explode_outer(F.col(\"del.wickets\")).alias(\"wkt\")\r\n    )\r\n)\r\n\r\n\r\nfielders_names = F.transform(F.col(\"wkt.fielders\"), lambda f: f.getField(\"name\"))\r\nfielders_joined = F.when(\r\n    F.col(\"wkt.fielders\").isNotNull(),\r\n    F.array_join(fielders_names, \", \")\r\n)\r\n\r\ndelivery_table_df = (\r\n    base_deliveries\r\n    .select(\r\n        \"match_type_number\",\r\n        \"innings_number\",\r\n        \"country\",\r\n        \"over\",\r\n        \"ball_in_over\",\r\n        \"bowler\",\r\n        \"batter\",\r\n        \"non_striker\",\r\n        \"runs\",\r\n        \"extras\",\r\n        \"total\",\r\n        F.col(\"wkt.player_out\").alias(\"player_out\"),\r\n        F.col(\"wkt.kind\").alias(\"player_out_kind\"),\r\n        fielders_joined.alias(\"player_out_fielders\")\r\n    )\r\n    .join(\r\n        json_df.select(\r\n            F.col(\"_source\"),\r\n            F.col(\"stg_file_name\"),\r\n            F.col(\"stg_file_row_number\"),\r\n            F.col(\"stg_file_hashkey\"),\r\n            F.col(\"stg_modified_ts\"),\r\n            F.col(\"ingest_date\"),\r\n            F.col(\"info.match_type_number\").alias(\"mt_for_join\")\r\n        ).dropDuplicates([\"_source\", \"mt_for_join\"]),\r\n        on=(F.col(\"match_type_number\") == F.col(\"mt_for_join\")),\r\n        how=\"left\"\r\n    )\r\n    .drop(\"mt_for_join\", \"_source\")\r\n)\r\n\r\n'''\r\n# ---------- WRITE CSVs ----------\r\ndef write_csv(df, subpath):\r\n    out = df.coalesce(COALESCE) if COALESCE > 0 else df\r\n    (out.write\r\n        .mode(\"overwrite\")\r\n        .option(\"header\", \"true\")\r\n        .csv(OUTPUT_BASE + subpath))\r\n\r\nwrite_csv(match_table_df, \"match_table/\")\r\nwrite_csv(player_table_df, \"player_table/\")\r\nwrite_csv(delivery_table_df, \"delivery_table/\")\r\n'''\r\n# ---------- WRITE PARQUET ----------\r\n\r\n# figure out this run's date from the data you just built\r\nrun_date = json_df.agg(F.max(\"ingest_date\").alias(\"d\")).collect()[0][\"d\"]\r\n\r\ndef write_parquet_history(df, subpath):\r\n    out = df.coalesce(COALESCE) if COALESCE > 0 else df\r\n    (out.write\r\n        .format(\"parquet\")\r\n        .mode(\"append\")\r\n        .partitionBy(\"ingest_date\")\r\n        .save(OUTPUT_BASE + subpath))\r\n\r\ndef write_parquet_latest(df, subpath):\r\n    # keep only this run's rows and overwrite a fixed 'latest/' alias\r\n    today = df.filter(F.col(\"ingest_date\") == F.lit(run_date))\r\n    out = today.coalesce(COALESCE) if COALESCE > 0 else today\r\n    (out.write\r\n        .format(\"parquet\")\r\n        .mode(\"overwrite\")\r\n        .save(OUTPUT_BASE + subpath + \"latest/\"))\r\n\r\n# write both history and latest for each table\r\nfor sub in [\"match_table/\", \"player_table/\", \"delivery_table/\"]:\r\n    write_parquet_history(eval(sub.split('/')[0] + \"_df\"), sub)\r\n    write_parquet_latest(eval(sub.split('/')[0] + \"_df\"), sub)\r\n"
}